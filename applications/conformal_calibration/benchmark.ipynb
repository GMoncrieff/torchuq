{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-slide",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_label = 0\n",
    "gpu = 0\n",
    "use_adv = False\n",
    "network_name = 'fcdrop'\n",
    "network_size = 128\n",
    "log_root = 'log/'\n",
    "normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from functools import partial\n",
    "import pickle \n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchuq.transform.conformal import ConformalCalibrator\n",
    "from torchuq.transform.naive import *\n",
    "from torchuq.metric.distribution import *\n",
    "from torchuq.dataset import regression\n",
    "\n",
    "device = torch.device('cuda:%d' % gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    run_name = 'network=%s-%d-use_adv=%r-normalize=%r/run_label=%d' % (network_name, network_size, use_adv, normalize, run_label)\n",
    "    log_dir = os.path.join(log_root, run_name)\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        break\n",
    "    run_label += 1\n",
    "    break\n",
    "    \n",
    "print(\"Run number = %d\" % run_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchuq.models.network import NetworkFC, NetworkFCDrop\n",
    "network_classes = {'fc': NetworkFC, 'fcdrop': NetworkFCDrop}\n",
    "def make_network(x_dim, out_dim):\n",
    "    return network_classes[network_name](x_dim=x_dim, out_dim=out_dim, feat_dim=network_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function uses global variables network_class, run_label and device\n",
    "def evaluate_performance(train_model, prediction_type, repeat=1, use_adv=False, debug_mode=False, verbose=False):\n",
    "    results = {}\n",
    "    \n",
    "    for name in regression.dataset_names:\n",
    "        results[name] = {'crps': torch.zeros(repeat), 'std': torch.zeros(repeat), 'nll': torch.zeros(repeat), 'ece': torch.zeros(repeat)}\n",
    "        for rep in range(repeat):\n",
    "            train_dataset, val_dataset, test_dataset = regression.get_regression_datasets(name, split_seed=run_label+rep, val_fraction=0.2, \n",
    "                                                                                          test_fraction=0.2, normalize=normalize, verbose=verbose)\n",
    "            if debug_mode and len(train_dataset) > 1500:\n",
    "                continue\n",
    "            val_labels = val_dataset[:][1]\n",
    "            test_labels = test_dataset[:][1]\n",
    "            prediction_val, prediction_test = train_model(train_dataset, val_dataset, test_dataset, use_adv=use_adv, verbose=verbose, network_class=make_network, device=device)\n",
    "            \n",
    "            calibrator = ConformalCalibrator(input_type=prediction_type, interpolation='linear')\n",
    "            calibrator.train(prediction_val, val_labels)\n",
    "            final_prediction = calibrator(prediction_test)\n",
    "\n",
    "            # Evaluate some performance metrics\n",
    "    #         plot_reliability_diagram(final_prediction, test_labels)\n",
    "    #         plot_density(final_prediction, test_labels)\n",
    "            \n",
    "            results[name]['crps'][rep] = compute_crps(final_prediction, test_labels).mean()\n",
    "            results[name]['std'][rep] = compute_std(final_prediction).mean()\n",
    "            results[name]['nll'][rep] = compute_nll(final_prediction, test_labels).mean()\n",
    "            results[name]['ece'][rep] = compute_ece(final_prediction, test_labels, debiased=True)\n",
    "        print(\"Finished dataset %s, nll=%.4f, std=%.4f, crps=%.4f, ece=%.4f\" % \n",
    "              (name, results[name]['nll'].mean(), results[name]['std'].mean(), results[name]['crps'].mean(), results[name]['ece'].mean()))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_learner import train_point, train_normal, train_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(train_dataset, val_dataset, test_dataset, n_ensemble=10, use_adv=False):\n",
    "    predictions_val = []\n",
    "    predictions_test = []\n",
    "    for n in range(n_ensemble):\n",
    "        val, test = train_normal(train_dataset, val_dataset, test_dataset, use_adv=use_adv, verbose=False)\n",
    "        predictions_val.append(val)\n",
    "        predictions_test.append(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_point = evaluate_performance(train_point, 'point', use_adv=use_adv)\n",
    "with open(os.path.join(log_dir, 'results_point.pickle'), 'wb') as handle:\n",
    "    pickle.dump(results_point, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-offset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dist = evaluate_performance(train_normal, 'distribution', use_adv=use_adv)\n",
    "with open(os.path.join(log_dir, 'results_dist.pickle'), 'wb') as handle:\n",
    "    pickle.dump(results_dist, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_quantile = []\n",
    "for i in [2, 3, 5, 7, 10, 15, 20]:\n",
    "    results_quantile = evaluate_performance(partial(train_quantile, n_quantiles=i), 'quantile', use_adv=use_adv)\n",
    "    with open(os.path.join(log_dir, 'results_quantile_%d.pickle' % i), 'wb') as handle:\n",
    "        pickle.dump(results_quantile, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
