{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developing-absolute",
   "metadata": {},
   "source": [
    "# Tutorial 1.b: Learning Uncertainty Representations from Data with Gradient Descent\n",
    "\n",
    "In the previous tutorial we demonstrated several types of predictions and metrics to measure their quality. Naturally, given a quality measurement we can also optimize the predictions to maximize the quality. This is the goal of this tutorial. \n",
    "\n",
    "\n",
    "We will use a standard deep learning pipeline. We define a neural network that inputs the features, and outputs a tensor with the correct shape (for example, if want a quantile prediction, the network should output a tensor of shape ``[batch_size, n_quantiles]``). We use torchuq metrics as a learning objective. One of the key property of torchuq is that most metrics are differentiable. Therefore, they can be used as objective functions and optimized with gradient descent. To see which metrics are differentiable see the reference list in [TBD]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-major",
   "metadata": {},
   "source": [
    "### Setting up the Environment \n",
    "\n",
    "We first setup the environment of the tutorial. First load the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preceding-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import torchuq \n",
    "\n",
    "device = torch.device('cuda:0')  \n",
    "# device = torch.device('cpu')  # Use this option if you do not have GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-mississippi",
   "metadata": {},
   "source": [
    "**Dataset**: We will use the UCI boston dataset. For your convenience torchuq includes a large collection of fairly small datasets with a single interface ``torchuq.dataset.regression.get_regression_datasets`` or ``torchuq.dataset.classification.get_classification_datasets``. All the data files are included with the repo, so you should be able to use these datasets out-of-the-box. For a list of available datasets see [link](https://github.com/ShengjiaZhao/torchuq/tree/main/torchuq/dataset). To access a dataset call \n",
    "\n",
    "`` train_dataset, val_dataset, test_dataset = torchuq.dataset.regression.get_regression_datasets(dataset_name, val_fraction=0.2, test_fraction=0.2) ``\n",
    "\n",
    "You can split the data into train/val/test by setting non-zero values to the arguments ``val_fraction`` and ``test_fraction``. The return values are pytorch Dataset instances, which can be conviniently used with pytorch dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "democratic-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset boston....\n",
      "Splitting into train/val/test with 405/101/0 samples\n",
      "Done loading dataset boston\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, _ = torchuq.dataset.regression.get_regression_datasets('boston', val_fraction=0.2, test_fraction=0.0, verbose=True)\n",
    "x_dim = len(train_dataset[0][0])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-adelaide",
   "metadata": {},
   "source": [
    "**Prediction Model**: For simplicity, we use a 3 layer fully connected neural network as the prediction function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "diagnostic-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkFC(nn.Module):\n",
    "    def __init__(self, x_dim, out_dim=1, num_feat=30):\n",
    "        super(NetworkFC, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, num_feat)\n",
    "        self.fc2 = nn.Linear(num_feat, num_feat)\n",
    "        self.fc3 = nn.Linear(num_feat, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc2(F.leaky_relu(self.fc1(x))))\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-artwork",
   "metadata": {},
   "source": [
    "### Learning a Point Prediction\n",
    "\n",
    "Learning a point prediction follows very standard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-merchant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchuq.metric.point import compute_l2_loss\n",
    "\n",
    "net = NetworkFC(x_dim, out_dim=1).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:    # Evaluate the validation performance\n",
    "        with torch.no_grad():  \n",
    "            val_x, val_y = val_dataset[:]\n",
    "            pred_val = net(val_x.to(device)).flatten()   # The network outputs an array of shape [batch_size, 1], must flatten to turn into point prediction\n",
    "            loss = compute_l2_loss(pred_val, val_y.to(device))\n",
    "            print(\"Epoch %d, loss=%.4f\" % (epoch, loss))\n",
    "    \n",
    "    for i, (bx, by) in enumerate(train_loader):  # Standard pytorch training loop\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(bx.to(device)).flatten()\n",
    "        loss = compute_l2_loss(pred, by.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Record the point predictions on the validation set\n",
    "# val_x, val_y = val_dataset[:]\n",
    "# predictions_point = net(val_x.to(device)).flatten().cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-desire",
   "metadata": {},
   "source": [
    "### Learning a Probability Prediction\n",
    "\n",
    "There is more flexibility with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "asian-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.6168\n",
      "Epoch 10, loss=0.5125\n",
      "Epoch 20, loss=0.4510\n",
      "Epoch 30, loss=0.4199\n",
      "Epoch 40, loss=0.4008\n",
      "Epoch 50, loss=0.3834\n",
      "Epoch 60, loss=0.3589\n",
      "Epoch 70, loss=0.3338\n",
      "Epoch 80, loss=0.3075\n",
      "Epoch 90, loss=0.2873\n"
     ]
    }
   ],
   "source": [
    "from torchuq.metric.distribution import compute_crps, compute_nll\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "net = NetworkFC(x_dim, out_dim=2).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:    # Evaluate the validation performance\n",
    "        with torch.no_grad():  \n",
    "            val_x, val_y = val_dataset[:]\n",
    "            pred_raw = net(val_x.to(device)) \n",
    "            pred_val = Normal(loc=pred_raw[:, 0], scale=pred_raw[:, 1].abs())\n",
    "            loss = compute_crps(pred_val, val_y.to(device)).mean()\n",
    "            # loss = compute_nll(pred_val, val_y.to(device)).mean()   \n",
    "            print(\"Epoch %d, loss=%.4f\" % (epoch, loss))\n",
    "    \n",
    "    for i, (bx, by) in enumerate(train_loader):  # Standard pytorch training loop\n",
    "        optimizer.zero_grad()\n",
    "        pred_raw = net(bx.to(device)) \n",
    "        pred = Normal(loc=pred_raw[:, 0], scale=pred_raw[:, 1].abs())\n",
    "        loss = compute_crps(pred, by.to(device)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# val_x, val_y = val_dataset[:]\n",
    "# pred_raw = net(val_x.to(device)).cpu().detach()\n",
    "# predictions_distribution = Normal(loc=pred_raw[:, 0], scale=pred_raw[:, 1].abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-sierra",
   "metadata": {},
   "source": [
    "### Learning a Quantile Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchuq.metric.quantile import compute_pinball_loss\n",
    "\n",
    "# Train quantile loss\n",
    "net = NetworkFC(x_dim, out_dim=10).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:    # Evaluate the validation performance\n",
    "        with torch.no_grad():  \n",
    "            val_x, val_y = val_dataset[:]\n",
    "            pred_val = net(val_x.to(device))\n",
    "            loss = compute_pinball_loss(pred_val, val_y.to(device))\n",
    "            print(\"Epoch %d, loss=%.4f\" % (epoch, loss))\n",
    "    \n",
    "    for i, (bx, by) in enumerate(train_loader):  # Standard pytorch training loop\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(bx.to(device))\n",
    "        loss = compute_pinball_loss(pred, by.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Record the quantile predictions on the validation set\n",
    "val_x, val_y = val_dataset[:]\n",
    "# predictions_quantile = torch.cummax(net(val_x.to(device)), dim=1)[0].cpu().detach()\n",
    "predictions_quantile = net(val_x.to(device)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchuq.metric import quantile\n",
    "quantile.plot_quantiles(torch.sort(predictions_quantile, dim=1)[0], val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchuq.metric.distribution import compute_nll\n",
    "from torch.distributions.normal import Normal\n",
    "from torchuq.transform.naive import quantile_to_distribution\n",
    "\n",
    "net = NetworkFC(x_dim, out_dim=20).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:    # Evaluate the validation performance\n",
    "        with torch.no_grad():  \n",
    "            val_x, val_y = val_dataset[:]\n",
    "            pred_raw = net(val_x.to(device)) \n",
    "            pred_val = quantile_to_distribution(pred_raw)\n",
    "\n",
    "            loss = compute_nll(pred_val, val_y.to(device)).mean()   \n",
    "            print(\"Epoch %d, loss=%.4f\" % (epoch, loss))\n",
    "    \n",
    "    for i, (bx, by) in enumerate(train_loader):  # Standard pytorch training loop\n",
    "        optimizer.zero_grad()\n",
    "        pred_raw = net(bx.to(device)) \n",
    "        pred_val = quantile_to_distribution(pred_raw)\n",
    "        loss = compute_nll(pred_val, by.to(device)).mean() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "val_x, val_y = val_dataset[:]\n",
    "pred_raw = net(val_x.to(device)).cpu().detach()\n",
    "predictions_distribution = quantile_to_distribution(pred_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchuq.metric.distribution import plot_density, plot_reliability_diagram\n",
    "# Record the quantile predictions on the validation set\n",
    "plot_density(predictions_distribution, val_y)\n",
    "plt.show()\n",
    "plot_reliability_diagram(predictions_distribution, val_y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the predictions we have learned in this tutorial. These are the predictions used in the first tutorial\n",
    "torch.save({'labels': val_y.flatten().cpu(),\n",
    "            'predictions_point': predictions_point,\n",
    "            'predictions_quantile': predictions_quantile,\n",
    "            'predictions_distribution': predictions_distribution,\n",
    "            'predictions_interval': torch.cat([predictions_quantile[:, :1], predictions_quantile[:, -1:]], dim=1)\n",
    "            }, 'pretrained/boston_pretrained.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
